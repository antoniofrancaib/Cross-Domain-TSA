{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e15a1c1c-2b2c-402c-8db7-b66dadef6f1b",
   "metadata": {},
   "source": [
    "# Cross-Domain Sentiment Classification with Domain-Adaptive Neural Networks\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "Sentiment analysis, the computational study of opinions expressed in text, has vast applications in understanding customer feedback, social media monitoring, and opinion mining. However, the performance of sentiment analysis models can significantly drop when applied to a new domain due to the domain discrepancy. This project aims to tackle this challenge using domain adaptation techniques in neural networks.\n",
    "\n",
    "## Objective\n",
    "\n",
    "The primary objective of this project is to develop a neural network capable of adapting the knowledge from one domain and effectively applying it to a different domain. Specifically, we will train our model on the IMDB movie review dataset and adapt it to analyze sentiments in the YELP restaurant review dataset.\n",
    "\n",
    "## Approach\n",
    "\n",
    "(TO WRITE after finishing the code)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4ca0c0-5b71-457b-87a1-7fc71606249c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Initialize Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa24857-51b0-4fa2-b287-0068b5565822",
   "metadata": {},
   "source": [
    "## IMDB Data ∼ Domain Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52b8afef-1e2e-49d3-bc19-089d4290f25e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6b3772a-2d8d-4876-b681-838de0e6a709",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_df = pd.read_csv('IMDB Dataset.csv')\n",
    "imdb_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74af136e-59e8-44c7-b87b-f1134df475c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment\n",
      "positive    25000\n",
      "negative    25000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "sentiment_counts_imdb = imdb_df['sentiment'].value_counts()\n",
    "print(sentiment_counts_imdb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d182c747-165c-49ae-ab57-b7e29ede4a2d",
   "metadata": {},
   "source": [
    "## Yelp Data ∼ Target Source\n",
    "\n",
    "We consider ratings of 4 and 5 stars as positive and ratings of 1 and 2 stars as negative. We discard 3-star reviews as they are neutral. Alternatively, we might include them in further explorations in one of the categories based on the needs of our analysis.\n",
    "\n",
    "https://www.kaggle.com/datasets/yelp-dataset/yelp-dataset/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "786475fe-b2a9-4024-8444-510eb77e895a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "data_file = open(\"yelp-dataset/yelp_academic_dataset_review.json\")\n",
    "review_df = []\n",
    "for line in data_file:\n",
    "    review_df.append(json.loads(line))\n",
    "yelp_df = pd.DataFrame(review_df)\n",
    "data_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b85d84ec-c813-4917-9375-2c51971b1988",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I've taken a lot of spin classes over the year...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Wow!  Yummy, different,  delicious.   Our favo...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cute interior and owner (?) gave us tour of up...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I am a long term frequent customer of this est...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Loved this tour! I grabbed a groupon and the p...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "1  I've taken a lot of spin classes over the year...  positive\n",
       "3  Wow!  Yummy, different,  delicious.   Our favo...  positive\n",
       "4  Cute interior and owner (?) gave us tour of up...  positive\n",
       "5  I am a long term frequent customer of this est...  negative\n",
       "6  Loved this tour! I grabbed a groupon and the p...  positive"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter out rows where 'stars' is 3 \n",
    "yelp_df = yelp_df[yelp_df['stars'] != 3.0].copy()\n",
    "\n",
    "yelp_df['sentiment'] = yelp_df['stars'].apply(lambda x: 'positive' if x >= 4 else 'negative')\n",
    "yelp_df = yelp_df.rename(columns={'text': 'review'})\n",
    "\n",
    "yelp_df = yelp_df[['review', 'sentiment']]\n",
    "\n",
    "yelp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "554d5984-ef71-40f6-bafe-07909e2ab103",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment\n",
      "positive    4684545\n",
      "negative    1613801\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ideally, we want symmetry here\n",
    "sentiment_counts = yelp_df['sentiment'].value_counts()\n",
    "print(sentiment_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338fedab-52fe-4317-aae0-096716d364bf",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. Initial Model Training with Source Domain (IMDB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29272c34-e42f-4aa1-a996-eb213dcd8c1c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Preprocessing for IMDB: \n",
    "<span style=\"color:red\">Status: </span> <span style=\"color:blue\">ALMOST FINISHED: </span>This includes text cleaning, tokenization, and padding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ca1110-6a63-4658-8871-c2d1d15c0e0c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Clean & Normalization \n",
    "Let's first clean the texts like removing stopwords, special characters, stemming, and lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4ee4d44-d986-4a2c-aebc-2c5011829908",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "import string\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000b7a18-d8f5-47cc-b8fd-6eb33d6d0718",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e65eac2a-8df6-49d1-9ebf-fadfddd8d28c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    # remove stop words like \"the\", \"is\", \"in\", \"on\", \"and\", \"but\", etc. \n",
    "    # the focus is on the more meaningful words that give insight into the content.\n",
    "    stop_words = stopwords.words('english')\n",
    "    words = text.split()\n",
    "    filtered_sentence = ''\n",
    "    for word in words:\n",
    "        if word not in stop_words:\n",
    "            filtered_sentence = filtered_sentence + word + ' '\n",
    "    return filtered_sentence\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    table = str.maketrans('','',string.punctuation)\n",
    "    words = text.split()\n",
    "    filtered_sentence = ''\n",
    "    for word in words:\n",
    "        word = word.translate(table)\n",
    "        filtered_sentence = filtered_sentence + word + ' '\n",
    "    return filtered_sentence\n",
    "\n",
    "def normalize_text(text):\n",
    "    text = text.lower()\n",
    "    # get rid of urls\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    # get rid of non words and extra spaces\n",
    "    text = re.sub('\\\\W', ' ', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    text = re.sub('^ ', '', text)\n",
    "    text = re.sub(' $', '', text)\n",
    "    return text\n",
    "\n",
    "def stemming(text):\n",
    "    ps = PorterStemmer()\n",
    "    words = text.split()\n",
    "    filtered_sentence = ''\n",
    "    for word in words:\n",
    "        word = ps.stem(word)\n",
    "        filtered_sentence = filtered_sentence + word + ' '\n",
    "    return filtered_sentence\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace(',',' , ')\n",
    "    text = text.replace('.',' . ')\n",
    "    text = text.replace('/',' / ')\n",
    "    text = text.replace('@',' @ ')\n",
    "    text = text.replace('#',' # ')\n",
    "    text = text.replace('?',' ? ')\n",
    "    text = normalize_text(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = stemming(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56568291-dba4-4d9c-a7f2-fb1577b6872b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "imdb_df['review'] = imdb_df['review'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553d6e21-f0cb-4c8e-9ffd-f195c9828c40",
   "metadata": {},
   "source": [
    "> Dataset Splitting and Labels Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c382f894-b942-49ea-abb8-9614b9b19d9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75e89d97-757d-47f5-8dfd-55c7d246ad1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train = imdb_df['review']\n",
    "y_train = imdb_df['sentiment']\n",
    "\n",
    "X_test = yelp_df['review']\n",
    "y_test = yelp_df['sentiment']\n",
    "\n",
    "one = OneHotEncoder()\n",
    "y_train = one.fit_transform(np.asarray(y_train).reshape(-1,1)).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52920d6d-71df-4b36-9d54-ca35f473116c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Tokenization and Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c9073e-3845-4e89-89f6-99fac84ffad8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install torch==2.0.0 torchtext==0.15.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c075c29b-bfa2-403c-a28c-66eb2c2b5b15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c31506f6-5a0b-4345-b4e1-f72906ae0d98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for text in data_iter:\n",
    "        yield tokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d86a73eb-dc3a-4993-b642-630833fccbfa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "max_length = 50\n",
    "padding_value = 0  # Index for <pad> token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa1bffa1-7449-41e5-820b-584397968e8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vocab = build_vocab_from_iterator(yield_tokens(X_train), specials=['<unk>', '<pad>', '<OOV>'], max_tokens=vocab_size)\n",
    "vocab.set_default_index(vocab['<OOV>'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "177bc9eb-8b19-4efb-9012-c54f774c2b17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train_tokenized = [torch.tensor(vocab(tokenizer(sentence))) for sentence in X_train]\n",
    "X_test_tokenized = [torch.tensor(vocab(tokenizer(sentence))) for sentence in X_test]\n",
    "\n",
    "X_train_padded = pad_sequence(X_train_tokenized, batch_first=True, padding_value=padding_value).to(torch.int64)\n",
    "X_test_padded = pad_sequence(X_test_tokenized, batch_first=True, padding_value=padding_value).to(torch.int64)\n",
    "\n",
    "X_train_padded = X_train_padded[:, :max_length]\n",
    "X_test_padded = X_test_padded[:, :max_length]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34655db-af99-4234-89c5-39771bf5e4c0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Tensorflow approach\n",
    "NO NEED TO RUN THIS PART!\n",
    "(notebook I found forinspiration: https://www.kaggle.com/code/antoniofranca/sentiment-analysis-on-imdb-movie-reviews/edit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5132aa2b-3837-4250-aea4-f88f9e272e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# important libraries for deep learning\n",
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "# for tokenizing texts\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# for text padding and truncating\n",
    "from tensorflow.keras.utils import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f645f3-d433-4112-942c-a5ce5f403d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# important properties\n",
    "vocab_size = 10000\n",
    "max_length = 50\n",
    "\n",
    "trunc_type = 'post'\n",
    "padding_type = 'post'\n",
    "oov_tok = '<OOV>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b4b21f-9443-4971-8a61-2e5423f8c3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tokenizer and fit on texts\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f5a5fe-18a4-4045-8884-8938bf9d8bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To Save conf execute this cell\n",
    "#Save Tokenizer Configuration\n",
    "import json \n",
    "import os \n",
    "\n",
    "tok_conf = tokenizer.to_json()\n",
    "\n",
    "with open('tok_conf.json', 'w') as outfile:\n",
    "    outfile.write(json.dumps(tok_conf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfbfec9-eedf-4810-b8fc-88c5a000cfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's Tokenize and pad texts\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "X_train = pad_sequences(X_train, maxlen=max_length,\n",
    "                         padding=padding_type,\n",
    "                         truncating=trunc_type)\n",
    "X_test = pad_sequences(X_test, maxlen=max_length,\n",
    "                         padding=padding_type,\n",
    "                         truncating=trunc_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73b5cad-abfd-4797-ba21-721e68357296",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089494ce-5930-416d-a9d6-fc8e94cbc51d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Build Model: \n",
    "<span style=\"color:red\">Status: </span> <span style=\"color:blue\">TO DO: </span> Design a neural network architecture suitable for sentiment analysis, e.g., LSTM, GRU, or even a transformer-based model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7a51f6f5-4602-45e3-ad45-89a948fa0f8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_train1d = np.array([int(i[1]) for i in y_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ad43ab98-f695-4387-a4d2-d8bf312766d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 1s 853us/step - loss: 28.8934 - accuracy: 0.5006 - val_loss: 2.5746 - val_accuracy: 0.4990\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 796us/step - loss: 2.3323 - accuracy: 0.5091 - val_loss: 1.5347 - val_accuracy: 0.5008\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 802us/step - loss: 1.3726 - accuracy: 0.5087 - val_loss: 1.1219 - val_accuracy: 0.5008\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 794us/step - loss: 1.1004 - accuracy: 0.5114 - val_loss: 1.1360 - val_accuracy: 0.5005\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 794us/step - loss: 0.8610 - accuracy: 0.5118 - val_loss: 0.8484 - val_accuracy: 0.5014\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 794us/step - loss: 0.7986 - accuracy: 0.5133 - val_loss: 0.8239 - val_accuracy: 0.4993\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 797us/step - loss: 0.7270 - accuracy: 0.5160 - val_loss: 0.7212 - val_accuracy: 0.5171\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 795us/step - loss: 0.6965 - accuracy: 0.5269 - val_loss: 0.7008 - val_accuracy: 0.5100\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 771us/step - loss: 0.6928 - accuracy: 0.5222 - val_loss: 0.7014 - val_accuracy: 0.5063\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 792us/step - loss: 0.6918 - accuracy: 0.5246 - val_loss: 0.6983 - val_accuracy: 0.4989\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f16aa750>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(256, activation='relu', input_shape=(50,)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "X_train_padded_numpy = X_train_padded.numpy()\n",
    "\n",
    "model.fit(X_train_padded_numpy, y_train1d, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408c4864-e042-447b-afa8-f80ee211d511",
   "metadata": {},
   "source": [
    "## Train Model on IMDB: \n",
    "<span style=\"color:red\">Status: </span> <span style=\"color:blue\">TO DO: </span> Using the IMDB dataset, train your model until it achieves satisfactory performance. This trained model captures the characteristics of the source domain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac57fa93-360b-4b03-89a4-0bf99d3995cd",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2. Domain Adaptation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a00895c-34dc-4e32-92d9-4bb51c26d6a3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Adversarial Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0bfd17",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "44d2ddc7-4696-4fd5-9edd-f42fc396a24f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c4107439-9aef-4a93-a4c5-b35c56ffefaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = X_train_padded\n",
    "y1 = [v[1] for v in y_train]\n",
    "\n",
    "x2 = X_test_padded[:50000]\n",
    "y2 = [0.0 if s == 'negative' else 1.0 for s in y_test[:50000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f283ef0b-e64c-4a2c-8ae9-afda9b00750d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "#Really not sure about this:\n",
    "# Define your neural network architecture\n",
    "class DomainAdaptationModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(DomainAdaptationModel, self).__init__()\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.classifier = nn.Linear(output_size, 1)\n",
    "        self.domain_classifier = nn.Linear(output_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.feature_extractor(x)\n",
    "        logits = self.classifier(features)\n",
    "        domain_logits = self.domain_classifier(features)\n",
    "        return logits, domain_logits\n",
    "\n",
    "# Set up your data loaders for source and target domains (x1, y1) and (x2, y2)\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 50\n",
    "hidden_size = 20\n",
    "output_size = 10\n",
    "lr = 0.001\n",
    "num_epochs = 10\n",
    "\n",
    "# Initialize model and optimizers\n",
    "model = DomainAdaptationModel(input_size, hidden_size, output_size)\n",
    "classifier_optimizer = optim.Adam(model.classifier.parameters(), lr=lr)\n",
    "domain_optimizer = optim.Adam(model.domain_classifier.parameters(), lr=lr)\n",
    "\n",
    "# Define loss functions\n",
    "classification_loss = nn.BCEWithLogitsLoss()\n",
    "domain_loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for x_source, y_source in source_dataloader:\n",
    "        x_source, y_source = Variable(x_source), Variable(y_source)\n",
    "\n",
    "        # Train classifier\n",
    "        classifier_optimizer.zero_grad()\n",
    "        logits, _ = model(x_source)\n",
    "        loss_classifier = classification_loss(logits, y_source)\n",
    "        loss_classifier.backward()\n",
    "        classifier_optimizer.step()\n",
    "\n",
    "        # Train domain classifier (minimize its accuracy)\n",
    "        domain_optimizer.zero_grad()\n",
    "        _, domain_logits = model(x_source)\n",
    "        domain_labels_source = torch.zeros_like(domain_logits)\n",
    "        loss_domain_source = domain_loss(domain_logits, domain_labels_source)\n",
    "\n",
    "        x_target, _ = target_dataloader.random_batch()  # Assuming you have a function to get batches\n",
    "        x_target = Variable(x_target)\n",
    "        _, domain_logits_target = model(x_target)\n",
    "        domain_labels_target = torch.ones_like(domain_logits_target)\n",
    "        loss_domain_target = domain_loss(domain_logits_target, domain_labels_target)\n",
    "\n",
    "        loss_domain = loss_domain_source + loss_domain_target\n",
    "        loss_domain.backward()\n",
    "        domain_optimizer.step()\n",
    "\n",
    "# Now, your model should have adapted to the target domain while maintaining good classification performance on the source domain.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56952d6e-a457-4c74-995d-fd4a174986e5",
   "metadata": {},
   "source": [
    "# 3. Fine-tuning on Target Domain (Optional but beneficial)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722363cb-95fa-4a11-aaf5-9c93a428ae9d",
   "metadata": {},
   "source": [
    "# 4. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a801ff-8c2a-4485-b439-301b68740d6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
